{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparison Figures â€” EMDAT Geocoding Validation\n",
    "\n",
    "This notebook generates comparison figures to evaluate geocoded event geometries against reference benchmarks. It:\n",
    "- Loads validation outputs produced by the pipeline (`output/` CSV files following a strict naming convention)\n",
    "- Merges metadata from the EMDAT archive (year, region, disaster type)\n",
    "- Produces histograms and summary bar charts (by region, year, and disaster type)\n",
    "\n",
    "Requirements:\n",
    "- `config.toml` must define paths: `path.validation_output_dir` and `path.emdat_archive_path`.\n",
    "- Validation outputs must be present in the `output/` directory and follow the expected filename pattern."
   ],
   "id": "ad5e773c5358712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import tomllib\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from validation.io import load_emdat_archive\n",
    "\n",
    "with open('config.toml', 'rb') as f:\n",
    "    config = tomllib.load(f)"
   ],
   "id": "1e8d91775c782957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plot styling\n",
    "Standardized Matplotlib and Seaborn styling for consistent figures in the paper/report.\n"
   ],
   "id": "a67a7d3612bc9902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MPL_CONFIG = {\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": \"8\",\n",
    "    \"axes.titlesize\": \"6\",\n",
    "    \"axes.labelsize\": \"6\",\n",
    "    \"xtick.labelsize\": \"6\",\n",
    "    \"ytick.labelsize\": \"6\",\n",
    "    \"legend.fontsize\": \"6\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"lines.linewidth\": .5,\n",
    "    \"xtick.major.size\": 3,\n",
    "    \"xtick.minor.size\": 2,\n",
    "    \"ytick.major.size\": 3,\n",
    "    \"ytick.minor.size\": 2,\n",
    "    \"xtick.major.width\": 0.5,\n",
    "    \"ytick.major.width\": 0.5,\n",
    "    \"xtick.minor.width\": 0.3,\n",
    "    \"ytick.minor.width\": 0.3,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"legend.frameon\": False,\n",
    "    \"axes.grid\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.bottom\": False,\n",
    "}\n",
    "mpl.rcParams.update(MPL_CONFIG)"
   ],
   "id": "ff560c5d9d8632bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## I/O and constants\n",
    "Set up paths, filename pattern, and plotting bins.\n"
   ],
   "id": "4bc06fb8228bbe5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "OUTPUT_DIR = Path(config['path']['validation_output_dir'])\n",
    "FILENAME_PATTERN = r'^([a-z]+_[a-z]+)_([a-z]+)_batch([1-5])(?:_dissolved)?\\.csv$'\n",
    "BINS = np.linspace(0, 1, 11)"
   ],
   "id": "f6c25996758fb10c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helper functions\n",
    "Filename validation and parsing utilities.\n"
   ],
   "id": "573ddd137839a5fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_output_filename(\n",
    "        filename: str | Path,\n",
    "        filename_pattern=FILENAME_PATTERN\n",
    ") -> bool:\n",
    "    \"\"\"Check if a filename matches the expected pattern.\"\"\"\n",
    "    if isinstance(filename, Path):\n",
    "        filename = filename.name\n",
    "    elif not isinstance(filename, str):\n",
    "        raise TypeError(\"Filename must be a string or Path object.\")\n",
    "    return bool(re.match(filename_pattern, filename))\n",
    "\n",
    "\n",
    "def _parse_output_filename(\n",
    "        filename: str | Path,\n",
    "        flexible_pattern=FILENAME_PATTERN\n",
    "):\n",
    "    \"\"\"Parse filename using flexible regex and extract components.\"\"\"\n",
    "    match = re.match(flexible_pattern, filename)\n",
    "    if match:\n",
    "        geom_source = match.group(1)  # e.g., \"llm_gadm\", etc.\n",
    "        benchmark = match.group(2)  # e.g., \"gaul\", \"gdis\"\n",
    "        batch_number = int(match.group(3))\n",
    "        dissolved = filename.endswith('_dissolved.csv')\n",
    "        return geom_source, benchmark, batch_number, dissolved\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_output_file_list(output_dir: str | Path):\n",
    "    \"\"\"Get a list of output files.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_files = [f.name for f in output_dir.glob('*.csv')]\n",
    "    assert all([validate_output_filename(fn) for fn in output_files])\n",
    "    return output_files\n",
    "\n",
    "\n",
    "def get_output_files_metadata(output_list: list[str]):\n",
    "    \"\"\"Get a dataframe of output files metadata.\"\"\"\n",
    "    metadata = [_parse_output_filename(fn) for fn in output_list]\n",
    "    return pd.DataFrame(\n",
    "        index=output_list,\n",
    "        columns=['geom_source', 'benchmark', 'batch_number', 'dissolved'],\n",
    "        data=metadata\n",
    "    )\n",
    "\n",
    "\n",
    "def get_output_data(output_list: list[str],\n",
    "                    output_dir: str | Path = OUTPUT_DIR):\n",
    "    \"\"\"Get a dataframe of output data.\"\"\"\n",
    "    output_data = pd.DataFrame()\n",
    "    for fn in output_list:\n",
    "        df_tmp = pd.read_csv(output_dir / fn)\n",
    "        output_data = pd.concat([output_data, df_tmp], axis=0)\n",
    "    return output_data.reset_index(drop=True)\n",
    "\n"
   ],
   "id": "9dc5e4a5fb119db7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load output file list and metadata\n",
    "Discover available CSV outputs and parse their metadata using the filename pattern.\n"
   ],
   "id": "2b2770b19ef0e916"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_list = get_output_file_list(OUTPUT_DIR)\n",
    "output_files_metadata = get_output_files_metadata(output_file_list)\n",
    "output_files_metadata"
   ],
   "id": "7ff3db9c01dec282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_list_dissolved = [i for i in output_file_list if \"dissolved\" in i]\n",
    "file_list_not_dissolved = [i for i in output_file_list if \"dissolved\" not in i]\n",
    "output_data = get_output_data(file_list_not_dissolved)\n",
    "output_data_dissolved = get_output_data(file_list_dissolved)\n",
    "print(f\"Rows (not dissolved): {len(output_data):,}\")\n",
    "print(f\"Rows (dissolved): {len(output_data_dissolved):,}\")"
   ],
   "id": "45339766e236b2b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Post-processing\n",
    "- For `llm_wiki`, use `b_contains_a` as `a_in_b` to keep the metric comparable.\n",
    "- Remove incomplete dissolves when the same `dis_no` appears dissolved across different batches.\n",
    "- Join with EMDAT metadata and harmonize labels.\n",
    "\n",
    "### post-processing wiki data"
   ],
   "id": "b807d604a052455a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_data.loc[\n",
    "    output_data['geom_type_a'] == 'llm_wiki',\n",
    "    'a_in_b'\n",
    "] = output_data.loc[\n",
    "    output_data['geom_type_a'] == 'llm_wiki',\n",
    "    'b_contains_a'\n",
    "].astype(float)"
   ],
   "id": "ee19473476d080c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove duplicates in dissolved comparative analysis\n",
    "\n",
    "For a few and marginal number of cases, some units associated with the same\n",
    "disno. were dissolved, while being dispatched in different batches.\n",
    "In these cases, the dissolve was then incomplete and the resulting statistic\n",
    "not relevant. Hence, we remove these cases here:"
   ],
   "id": "a269d57f535ca310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_data_dissolved.drop_duplicates(\n",
    "    subset=['dis_no', 'geom_type_a', 'geom_type_b'],\n",
    "    keep=False,\n",
    "    inplace=True\n",
    ")\n",
    "print(f\"Rows (dissolved): {len(output_data_dissolved):,}\")"
   ],
   "id": "840899aa79fff30e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Merge with EMDAT metadata and harmonize labels\n",
    "- Harmonize `geom_type_a` naming: replace `gdis_gadm` with `GDIS`.\n",
    "- Merge EMDAT archive columns (year, region, subregion, country, disaster type) for grouping and summaries.\n"
   ],
   "id": "8e3cfb86ba595017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Harmonize geom_type_a values across both DataFrames\n",
    "replacements = {\n",
    "    \"gdis_gadm\": \"GDIS\",\n",
    "    \"llm_gadm\": \"LLMGeoDis (GADM)\",\n",
    "    \"llm_osm\": \"LLMGeoDis (OSM)\",\n",
    "    \"llm_wiki\": \"LLMGeoDis (Wiki)\"\n",
    "}\n",
    "\n",
    "output_data['geom_type_a'] = output_data['geom_type_a'].replace(replacements)\n",
    "output_data_dissolved['geom_type_a'] = output_data_dissolved[\n",
    "    'geom_type_a'].replace(replacements)\n"
   ],
   "id": "6f7dc8a96d3acd71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add year & region\n",
    "df_emdat = load_emdat_archive(\n",
    "    config[\"path\"][\"emdat_archive_path\"],\n",
    "    use_columns=[\"DisNo.\", \"Start Year\", \"Country\", \"Region\", \"Subregion\",\n",
    "                 \"Disaster Type\"]\n",
    ").rename(columns={\"DisNo.\": \"dis_no\"})\n",
    "\n",
    "output_data = pd.merge(\n",
    "    output_data, df_emdat, how=\"left\", left_on=\"dis_no\", right_on=\"dis_no\"\n",
    ")\n",
    "output_data_dissolved = pd.merge(\n",
    "    output_data_dissolved,\n",
    "    df_emdat,\n",
    "    how=\"left\",\n",
    "    left_on=\"dis_no\",\n",
    "    right_on=\"dis_no\"\n",
    ")"
   ],
   "id": "6db78f5fbaaeb89e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output_data",
   "id": "b991a3e66060c128"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary statistics\n",
    "Inspect a specific source/benchmark subset and basic distribution for a spot check.\n",
    "\n",
    "### A in B (not dissolved)"
   ],
   "id": "a049f93974fc8e03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a_in_b_summary = output_data.groupby(['geom_type_a', 'geom_type_b'])[\n",
    "    ['a_in_b']].describe(percentiles=[.1, .9])\n",
    "a_in_b_summary"
   ],
   "id": "d32cc8674cbf11bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "strictly_null_counts = output_data[output_data['a_in_b'] == 0].groupby(\n",
    "    ['geom_type_a', 'geom_type_b']\n",
    ").size().reset_index(name='strictly_null_count')\n",
    "\n",
    "total_counts = output_data.groupby(\n",
    "    ['geom_type_a', 'geom_type_b']).size().reset_index(name='total_count')\n",
    "\n",
    "null_frequency = pd.merge(strictly_null_counts, total_counts,\n",
    "                          on=['geom_type_a', 'geom_type_b'])\n",
    "null_frequency['frequency'] = null_frequency['strictly_null_count'] / \\\n",
    "                              null_frequency['total_count']\n",
    "null_frequency['not null frequency'] = 1 - null_frequency['frequency']\n",
    "\n",
    "below_10pct_counts = output_data[output_data['a_in_b'] < 0.1].groupby(\n",
    "    ['geom_type_a', 'geom_type_b']\n",
    ").size().reset_index(name='strictly_null_count')\n",
    "print(f\"Below 10% of area covered by A in B:\")\n",
    "print(below_10pct_counts)\n",
    "print(4965/37253*100)\n",
    "null_frequency"
   ],
   "id": "8bc09580bcdcb4d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A in B (dissolved)",
   "id": "32437054fdb39b27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a_in_b_dis_summary = \\\n",
    "output_data_dissolved.groupby(['geom_type_a', 'geom_type_b'])[\n",
    "    ['a_in_b']].describe(percentiles=[.1, .9])\n",
    "a_in_b_dis_summary"
   ],
   "id": "41e6ee4158f4eb18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "strictly_null_counts = output_data_dissolved[\n",
    "    output_data_dissolved['a_in_b'] == 0].groupby(\n",
    "    ['geom_type_a', 'geom_type_b']\n",
    ").size().reset_index(name='strictly_null_count')\n",
    "\n",
    "total_counts = output_data_dissolved.groupby(\n",
    "    ['geom_type_a', 'geom_type_b']).size().reset_index(name='total_count')\n",
    "\n",
    "null_frequency = pd.merge(strictly_null_counts, total_counts,\n",
    "                          on=['geom_type_a', 'geom_type_b'])\n",
    "null_frequency['frequency'] = null_frequency['strictly_null_count'] / \\\n",
    "                              null_frequency['total_count']\n",
    "\n",
    "null_frequency"
   ],
   "id": "b82b9ceb2b0cdaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Jaccard (dissolved)",
   "id": "812b85e29689f907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "jaccard_dis_summary = \\\n",
    "output_data_dissolved.groupby(['geom_type_a', 'geom_type_b'])[\n",
    "    ['jaccard']].describe()\n",
    "jaccard_dis_summary"
   ],
   "id": "35292d342b839d90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "strictly_null_counts = output_data_dissolved[\n",
    "    output_data_dissolved['jaccard'] == 0].groupby(\n",
    "    ['geom_type_a', 'geom_type_b']\n",
    ").size().reset_index(name='strictly_null_count')\n",
    "\n",
    "total_counts = output_data_dissolved.groupby(\n",
    "    ['geom_type_a', 'geom_type_b']).size().reset_index(name='total_count')\n",
    "\n",
    "null_frequency = pd.merge(strictly_null_counts, total_counts,\n",
    "                          on=['geom_type_a', 'geom_type_b'])\n",
    "null_frequency['frequency'] = null_frequency['strictly_null_count'] / \\\n",
    "                              null_frequency['total_count']\n",
    "\n",
    "null_frequency"
   ],
   "id": "99da5eac5f6ac049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Figures",
   "id": "86e9e38c33d067fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmark statistics against EM-DAT GAUL",
   "id": "bba5cce5308936be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combinations = list(product(output_data['geom_type_a'].unique(),\n",
    "                            output_data['geom_type_b'].unique()))\n",
    "# Select only GAUL benchmark combinations to plot\n",
    "combinations_gaul = [(s, b) for (s, b) in combinations if b == 'GAUL']\n",
    "combinations_gaul"
   ],
   "id": "dd290897d521e527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig_row = {\n",
    "    'GAUL': 0,\n",
    "    'GDIS': 1\n",
    "}\n",
    "\n",
    "histplot_kwargs = {\n",
    "    'element': 'step',\n",
    "    'fill': False,\n",
    "    'bins': BINS,\n",
    "    'stat': 'percent',\n",
    "    'linewidth': 0.75\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, sharex=True, sharey=True,\n",
    "                         figsize=(16 / 2.54, 3 / 2.54))\n",
    "\n",
    "for source, benchmark in combinations_gaul:\n",
    "    [ax0, ax1, ax2, ax3] = axes  #[fig_row[benchmark], :]\n",
    "    ax0.set_title(\"a) A (not dissolved) in B\")  #(f\"{benchmark} (a in b)\")\n",
    "    ax1.set_title(\"b) A in B\")  #(f\"{benchmark} (a in b)\")\n",
    "    ax2.set_title(\"c) B in A\")  #(f\"{benchmark} (b in a)\")\n",
    "    ax3.set_title(\"d) Jaccard index\")  #(f\"{benchmark} (Jaccard)\")\n",
    "\n",
    "    subset_data = output_data[(output_data['geom_type_a'] == source) & (\n",
    "            output_data['geom_type_b'] == benchmark)]\n",
    "    subset_data_dissolved = output_data_dissolved[\n",
    "        (output_data_dissolved['geom_type_a'] == source) & (\n",
    "                output_data_dissolved['geom_type_b'] == benchmark)]\n",
    "    # print(f\"{source} vs {benchmark}\")\n",
    "    # print(subset_data.head())\n",
    "\n",
    "    sns.histplot(data=subset_data, x='a_in_b', ax=ax0, label=f'{source}',\n",
    "                 **histplot_kwargs)  #(n={len(subset_data):,})\n",
    "    if source != 'LLMGeoDis (Wiki)':\n",
    "        sns.histplot(data=subset_data_dissolved, x='a_in_b', ax=ax1,\n",
    "                     label=f'{source}',\n",
    "                     **histplot_kwargs)  # (n={len(subset_data_dissolved):,})\n",
    "        sns.histplot(data=subset_data_dissolved, x='b_in_a', ax=ax2,\n",
    "                     label=f'{source}',\n",
    "                     **histplot_kwargs)  # (n={len(subset_data_dissolved):,})\n",
    "        sns.histplot(data=subset_data_dissolved, x='jaccard', ax=ax3,\n",
    "                     label=f'{source}',\n",
    "                     **histplot_kwargs)  # (n={len(subset_data_dissolved):,})\n",
    "\n",
    "\n",
    "def _format_ax(ax):\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(.20))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(.10))\n",
    "    ax.grid(which='both', axis='y', linestyle='-', linewidth=.3,\n",
    "            color='lightgrey')\n",
    "    ax.set_ylim((-1, 100))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_position(('outward', 2))\n",
    "    ax.set(xlabel=\"Proportion of Area\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    _format_ax(ax)\n",
    "ax0.legend(loc='center left', ncol=4, title=None, frameon=False,\n",
    "           bbox_to_anchor=(1, 1.25))"
   ],
   "id": "6043c77c6e3e9770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regional analysis\n",
    "Summarize percent of high-overlap cases (>= 0.9) by region for `a_in_b` and `jaccard`.\n"
   ],
   "id": "d78d41987bfedcf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Quick regional analysis\n",
    "regions = output_data[\"Region\"].unique()\n",
    "\n",
    "results_regions = pd.DataFrame(columns=[\"Region\", \"a_in_b\", \"jaccard\"])\n",
    "for source, benchmark in combinations_gaul:\n",
    "    subset_data_dissolved = output_data_dissolved[\n",
    "        (output_data_dissolved['geom_type_a'] == source) & (\n",
    "                output_data_dissolved['geom_type_b'] == benchmark)]\n",
    "    for region in regions:\n",
    "        region_filter = subset_data_dissolved[\"Region\"] == region\n",
    "        a_in_b = round(sum(\n",
    "            subset_data_dissolved.loc[region_filter, \"a_in_b\"] >= 0.9) / sum(\n",
    "            subset_data_dissolved.loc[region_filter, \"a_in_b\"] >= 0), 2) * 100\n",
    "        jaccard = round(sum(\n",
    "            subset_data_dissolved.loc[region_filter, \"jaccard\"] >= 0.9) / sum(\n",
    "            subset_data_dissolved.loc[region_filter, \"jaccard\"] >= 0), 2) * 100\n",
    "\n",
    "        results_regions = pd.concat([results_regions, pd.DataFrame(\n",
    "            [[source, benchmark, region, a_in_b, jaccard]],\n",
    "            columns=[\"a\", \"b\", \"Region\", \"a_in_b\", \"jaccard\"])])\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, sharex=True, sharey=True,\n",
    "                               figsize=(16 / 2.54, 3 / 2.54))\n",
    "sns.barplot(data=results_regions, x=\"Region\", y=\"a_in_b\", hue=\"a\", ax=ax1)\n",
    "ax1.set(ylabel=\"Percent\", title=\"a in b\")\n",
    "\n",
    "sns.barplot(data=results_regions, x=\"Region\", y=\"jaccard\", hue=\"a\", ax=ax2,\n",
    "            legend=\"\")\n",
    "ax2.set(ylabel=\"Percent\", title=\"jaccard\")\n",
    "\n",
    "sns.move_legend(\n",
    "    ax1, \"lower center\",\n",
    "    bbox_to_anchor=(1, 1.1), ncol=4, title=None, frameon=False,\n",
    ")"
   ],
   "id": "333f5d728c65dd22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Yearly analysis\n",
    "Summarize percent of high-overlap cases (>= 0.9) by year for `a_in_b` and `jaccard`.\n"
   ],
   "id": "615d852a6b3443cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "years = output_data[\"Start Year\"].unique()\n",
    "results_years = pd.DataFrame(columns=[\"Year\", \"a_in_b\", \"jaccard\"])\n",
    "for source, benchmark in combinations_gaul:\n",
    "    subset_data_dissolved = output_data_dissolved[\n",
    "        (output_data_dissolved['geom_type_a'] == source) & (\n",
    "                output_data_dissolved['geom_type_b'] == benchmark)]\n",
    "    for year in years:\n",
    "        year_filter = subset_data_dissolved[\"Start Year\"] == year\n",
    "        a_in_b = round(\n",
    "            sum(subset_data_dissolved.loc[year_filter, \"a_in_b\"] >= 0.9) / max(\n",
    "                sum(subset_data_dissolved.loc[year_filter, \"a_in_b\"] >= 0),\n",
    "                0.00001), 2) * 100\n",
    "        jaccard = round(\n",
    "            sum(subset_data_dissolved.loc[year_filter, \"jaccard\"] >= 0.9) / max(\n",
    "                sum(subset_data_dissolved.loc[year_filter, \"jaccard\"] >= 0),\n",
    "                0.0001), 2) * 100\n",
    "\n",
    "        results_years = pd.concat([results_years, pd.DataFrame(\n",
    "            [[source, benchmark, year, a_in_b, jaccard]],\n",
    "            columns=[\"a\", \"b\", \"Year\", \"a_in_b\", \"jaccard\"])])\n",
    "results_years\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, sharex=True, sharey=True,\n",
    "                               figsize=(16 / 2.54, 3 / 2.54))\n",
    "sns.barplot(data=results_years, x=\"Year\", y=\"a_in_b\", hue=\"a\", ax=ax1, width=1)\n",
    "ax1.set(ylabel=\"Percent\", title=\"a in b\")\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(data=results_years, x=\"Year\", y=\"jaccard\", hue=\"a\", ax=ax2, width=1,\n",
    "            legend=\"\")\n",
    "ax2.set(ylabel=\"Percent\", title=\"jaccard\")\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.move_legend(\n",
    "    ax1, \"lower center\",\n",
    "    bbox_to_anchor=(1, 1.1), ncol=4, title=None, frameon=False,\n",
    ")\n"
   ],
   "id": "3f4d3df2678c75ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Disaster type analysis\n",
    "Summarize percent of high-overlap cases (>= 0.9) by disaster type for `a_in_b` and `jaccard`.\n"
   ],
   "id": "3af8df1578f5d52f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "disaster_types = output_data[\"Disaster Type\"].unique()\n",
    "results_disaster_types = pd.DataFrame(\n",
    "    columns=[\"Disaster Type\", \"a_in_b\", \"jaccard\"])\n",
    "for source, benchmark in combinations_gaul:\n",
    "    subset_data_dissolved = output_data_dissolved[\n",
    "        (output_data_dissolved['geom_type_a'] == source) & (\n",
    "                output_data_dissolved['geom_type_b'] == benchmark)]\n",
    "    for disaster_type in disaster_types:\n",
    "        disaster_type_filter = subset_data_dissolved[\n",
    "                                   \"Disaster Type\"] == disaster_type\n",
    "        a_in_b = round(sum(subset_data_dissolved.loc[\n",
    "                               disaster_type_filter, \"a_in_b\"] >= 0.9) / max(\n",
    "            sum(subset_data_dissolved.loc[disaster_type_filter, \"a_in_b\"] >= 0),\n",
    "            0.00001), 2) * 100\n",
    "        jaccard = round(sum(subset_data_dissolved.loc[\n",
    "                                disaster_type_filter, \"jaccard\"] >= 0.9) / max(\n",
    "            sum(subset_data_dissolved.loc[\n",
    "                    disaster_type_filter, \"jaccard\"] >= 0), 0.0001), 2) * 100\n",
    "\n",
    "        results_disaster_types = pd.concat([results_disaster_types,\n",
    "                                            pd.DataFrame([[source, benchmark,\n",
    "                                                           disaster_type,\n",
    "                                                           a_in_b, jaccard]],\n",
    "                                                         columns=[\"a\", \"b\",\n",
    "                                                                  \"Disaster Type\",\n",
    "                                                                  \"a_in_b\",\n",
    "                                                                  \"jaccard\"])])\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, sharex=True, sharey=True,\n",
    "                               figsize=(16 / 2.54, 3 / 2.54))\n",
    "sns.barplot(data=results_disaster_types, x=\"Disaster Type\", y=\"a_in_b\", hue=\"a\",\n",
    "            ax=ax1, width=1)\n",
    "ax1.set(ylabel=\"Percent\", title=\"a in b\")\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(data=results_disaster_types, x=\"Disaster Type\", y=\"jaccard\",\n",
    "            hue=\"a\", ax=ax2, width=1, legend=\"\")\n",
    "ax2.set(ylabel=\"Percent\", title=\"jaccard\")\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.move_legend(\n",
    "    ax1, \"lower center\",\n",
    "    bbox_to_anchor=(1, 1.1), ncol=4, title=None, frameon=False,\n",
    ")\n"
   ],
   "id": "8da6735097e8689"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
